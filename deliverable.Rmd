---
title: "Practice CA, MCA and Clustering"
author: "Marc Mendez & Joel Cantero"
date: "7 de abril de 2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
library(FactoMineR)
library(mice)
library(ggplot2)
library(dendextend)
library(calibrate)
library(factoextra)
library(fpc)
setwd("~/Desktop/UPC 18:19/S2 18:19/MVA/Homework 4")
```
##1. Read the PCA_quetaltecaen data.
```{r}
quetal <- read_delim("PCA_quetaltecaen.txt","\t", escape_double = FALSE, trim_ws = TRUE)
names(quetal)[7]<- "madrilenos"
quetal.data <- quetal[-1]
row.names(quetal.data) <- quetal$CCAA
```

The dataset PCA_quetaltecaen is written in a txt file, columns are spaced by tabulations. The files encoding is not the usual due to the fact that ?? is a strange char. To solve this we will leave madrile??os as madrilenos to avoid any problem with the ISO encodings.

##2. Perform a CA of this data. How many dimensions are significant?. Interpret the first factorial plan.

```{r}
ca.quetal <- CA(quetal.data)
```



```{r}
x <- as.data.frame(ca.quetal$eig)
x.eigenV <- x$eigenvalue
x.eigenVec <- x$`percentage of variance`
x.scree <- data.frame(Number = seq(1, length(x.eigenV)), Value = x.eigenV)
plot(x.scree, main="Scree Plot",
     xlab="Dimensions", ylab="Eigen Value" , ylim=c(0, max(x.eigenV) + 0.1*max(x.eigenV)), xlim=c(1, length(x.eigenV) + 0.5)) 
lines(x.scree$Value, col="blue")
textxy(x.scree$Number, x.scree$Value, round(x.scree$Value, 4), cex=1)
abline(h=0.002, col="red")
```


```{r}
sum(x.eigenVec[1:3])
```

##3. For the PCA_quetaltecaen data, compute the contribution of each cell to the total inertia, that is: (fij - fi.x f.j)^2/(fi.x f.j). Compute the percentage of inertia due to the diagonal cells.
```{r}
computeIntertiaPerCell <- function(K){
    F = K/sum(K)
    fi = rowSums(F)
    fj = colSums(F)
    
    contrib_cell = (F)
    # Contribution per cell to the total intertia
    for (i in seq(1, nrow(F))) {
        for(j in seq(1, ncol(F))) {
            
            prod = (fi[i] * fj[j])
            contrib_cell[i, j] = (((F[i, j] - prod)^2)/prod)
        }
    }
    return(contrib_cell)
}
contrib_cell = computeIntertiaPerCell(quetal.data)
```


```{r}
totalInertia = sum(contrib_cell)
totalEigenVal = sum(x.eigenV)
```

```{r}
sumDiagInertia = sum(diag(as.matrix(contrib_cell)))
(diagInertia = sumDiagInertia*100/totalInertia)
```

##4. Clearly, the overloaded diagonal of the data set influences the results obtained (the overall inertia is mainly due to this overload diagonal). Try to nullify this influence by imputing the diagonal values by the independence hypothesis values of the product of marginal probabilities (=n x fi.x f.j). Take into account that each imputation modifies the marginal, hence you need an iterative algorithm.

```{r}
df_2 = df
for (x in seq(1, 10)) {
  for (x in seq(1, nrow(df_2))) {
    n = sum(df_2)
    F2 = df_2/n
    fi2 = rowSums(F2)
    fj2 = colSums(F2)
    df_2[x,x] <- n * fi2[x] * fj2[x]    
  }
}

```

```{r}
contribs = computeIntertiaPerCell(df_2)
totalInertia2 = sum(contribs)
sumDiagInertia2 = sum(diag(as.matrix(contribs)))
(diagInertia2 = sumDiagInertia2*100/totalInertia2)
```

##5. Perform a new CA upon the quetaltecaen table, with the modified diagonal and interpret the results.

```{r}

ca.df2 = CA(df_2)
fviz_ca_biplot(ca.df2, repel = TRUE)
```

```{r}
t2 = as.data.frame(ca.df2$eig)
(eigenv2 = t2$eigenvalue)
eigenVec2 = t2$`percentage of variance`

totalEigenVal2 = sum(eigenv2)
```


##6. Read the file ???mca_car.csv??? containing the data and its dictionary about the cars and their characteristics found in specialized magazines. The final goal will be to find a model to predict the price of cars as function of its characteristics. First we will perform a visualization of the information contained in the dataset, then we will perform a clustering of cars. The data has been previously preprocessed to have it in categorical form.

##7. With the obtained data frame perform a Multiple Correspondence Analysis. Take the brand and price (either categorical or continuous) as supplementary variables, whereas the remaining ones are active.

##8. Interpret the first two obtained factors.

##9. Decide the number of significant dimensions that you retain (by subtracting the average eigenvalue and represent the new obtained eigenvalues in a new screeplot).

##10. Perform a hierarchical clustering with the significant factors, decide the number of final classes to obtain and perform a consolidation operation of the clustering.


##11. Using the function catdes interpret and name the obtained clusters and represent them in the first factorial display.